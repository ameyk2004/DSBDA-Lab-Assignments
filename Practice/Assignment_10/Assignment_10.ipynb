{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and download all\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3762\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "\n",
    "with open('text_01.txt') as file:\n",
    "    doc_1 = file.read()\n",
    "    \n",
    "print(len(doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize \n",
    "\n",
    "word_tokens = word_tokenize(doc_1)\n",
    "\n",
    "sent_tokens = sent_tokenize(doc_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Between',\n",
       " '2016',\n",
       " 'and',\n",
       " '2019',\n",
       " ',',\n",
       " 'the',\n",
       " 'state',\n",
       " 'forest',\n",
       " 'department',\n",
       " 'under']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Between 2016 and 2019, the state forest department under the\\xa0BJP\\xa0government had launched Green Maharashtra drive with an aim to plant 50 crore trees across the state in the four-year period.',\n",
       " 'In October 2019, the government had claimed it had surpassed the target by planting 33 crore trees in July-September 2019.',\n",
       " 'The Indian Express\\xa0had found that non-forest agencies — such as gram panchayats — which were tasked with planting trees had not uploaded the mandatory audio-visual proof of the tree plantation drives on the specially created portal.',\n",
       " 'In Pune Revenue Division, it was claimed the gram panchayats planted 1.7 crore saplings; however, no evidence was uploaded for 87 per cent (1.49 crore) saplings.',\n",
       " 'Also, out of the 59 government agencies involved in the drive as many as 38 had not submitted survival reports about the saplings.']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word Length : 663\n",
      "After Stop Word Removal Length : 454\n",
      "['Between', '2016', '2019', ',', 'state', 'forest', 'department', 'BJP', 'government', 'launched']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "print(f\"Original Word Length : {len(word_tokens)}\")\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "print(f\"After Stop Word Removal Length : {len(word_tokens)}\")\n",
    "\n",
    "print(word_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'four-year': 'four-year',\n",
       " 'period': 'period',\n",
       " '.': '.',\n",
       " 'In': 'in',\n",
       " 'October': 'octob',\n",
       " '2019': '2019',\n",
       " ',': ',',\n",
       " 'government': 'govern',\n",
       " 'claimed': 'claim',\n",
       " 'surpassed': 'surpass'}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(w) for w in word_tokens]\n",
    "\n",
    "dict(zip(word_tokens[20:30], stemmed_tokens[20:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'four-year': 'four-year',\n",
       " 'period': 'period',\n",
       " '.': '.',\n",
       " 'In': 'In',\n",
       " 'October': 'October',\n",
       " '2019': '2019',\n",
       " ',': ',',\n",
       " 'government': 'government',\n",
       " 'claimed': 'claimed',\n",
       " 'surpassed': 'surpassed'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lem_tokens = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "\n",
    "dict(zip(word_tokens[20:30], lem_tokens[20:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "\n",
    "tf = FreqDist(word_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DjangoTutorials-ELbGRrFZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

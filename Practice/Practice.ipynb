{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Data Wrangling I\n",
    "\n",
    "#### **Objective**:\n",
    "This assignment aims to practice basic data wrangling techniques with **pandas**, focusing on cleaning and preprocessing a dataset. Key tasks include handling missing values, converting data types, and transforming categorical variables into numeric ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Steps to Accomplish**:\n",
    "\n",
    "1. **Load Dataset into a pandas DataFrame**:\n",
    "   - First, the dataset is loaded into a **pandas DataFrame** from a file (e.g., CSV or Excel). This allows us to easily manipulate and explore the data.\n",
    "\n",
    "2. **Inspect the First Few Rows**:\n",
    "   - Use the `head()` function to display the first few rows of the dataset, giving you a quick look at the structure of the data.\n",
    "\n",
    "3. **Check the Dataset's Information**:\n",
    "   - The `info()` function provides a concise summary of the DataFrame, including the number of non-null values, data types, and column names.\n",
    "\n",
    "4. **Convert Data Types to Numeric**:\n",
    "   - If any columns should be numeric but are stored as objects (e.g., strings), we convert them using pandas' `pd.to_numeric()` function.\n",
    "\n",
    "5. **Describe the Dataset**:\n",
    "   - The `describe()` function provides summary statistics for numeric columns, helping identify patterns, trends, or anomalies in the data.\n",
    "\n",
    "6. **Check for Missing Values**:\n",
    "   - To identify missing values, use `isnull()` which will show how many missing entries are in each column.\n",
    "\n",
    "7. **Handle Missing Values**:\n",
    "   - For missing data, you can either fill it with the **mean** of the column or use **forward fill** (`ffill`) to propagate previous values forward.\n",
    "\n",
    "8. **Rename Columns**:\n",
    "   - It's a good practice to rename columns to more meaningful names if they are unclear, using the `rename()` function.\n",
    "\n",
    "9. **Check Dataset Dimensions**:\n",
    "   - Use the `shape` attribute to check the dimensions of the dataset (i.e., the number of rows and columns).\n",
    "\n",
    "10. **Convert Categorical Variables to Numeric**:\n",
    "    - **LabelEncoder** from **sklearn.preprocessing** is used to convert categorical variables into numeric values. This is necessary for machine learning models, which require numerical input.\n",
    "    - Example: If a column contains categories like 'Male' and 'Female', **LabelEncoder** will convert them into numeric values such as 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **Theory and Concepts Covered**:\n",
    "\n",
    "- **Data Wrangling**: The process of cleaning and transforming raw data into a usable format for analysis. This includes handling missing values, renaming columns, and changing data types.\n",
    "\n",
    "- **Handling Missing Data**: Missing values are a common problem in real-world data. Strategies to handle missing data include replacing missing values with the column's mean (for numerical data) or using techniques like **forward filling**.\n",
    "\n",
    "- **Converting Data Types**: Data often comes in the wrong type (e.g., text instead of numbers). Converting the data to the correct type is crucial for analysis and modeling. Numeric columns are particularly important in machine learning.\n",
    "\n",
    "- **Categorical to Numeric Conversion**: Machine learning algorithms generally require numerical input. **LabelEncoder** is a simple tool for converting categorical data (e.g., 'Yes', 'No') into numeric values (e.g., 1, 0). This helps the model process categorical information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Potential Questions and Answers**:\n",
    "\n",
    "**Q1: Why do we need to convert categorical variables into numeric values?**  \n",
    "**A1:** Many machine learning models require numerical input. Categorical variables, like 'Male' and 'Female' or 'Red' and 'Blue', are typically encoded into numeric values to allow the algorithm to interpret the data.\n",
    "\n",
    "**Q2: What are the different methods to handle missing values?**  \n",
    "**A2:** Common methods for handling missing values include:\n",
    "   - **Forward fill (ffill)**: Fills missing values with the previous non-null value.\n",
    "   - **Backward fill (bfill)**: Fills missing values with the next non-null value.\n",
    "   - **Filling with mean/median**: Replaces missing values with the mean or median of the column.\n",
    "   - **Dropping rows/columns**: If the missing data is too significant, rows or columns with missing values can be removed.\n",
    "\n",
    "**Q3: What is the difference between `LabelEncoder` and **One-Hot Encoding**?**  \n",
    "**A3:**  \n",
    "   - **LabelEncoder** assigns each category a unique integer. It's simple and works well for ordinal data (data with a natural order, like 'Low', 'Medium', 'High').\n",
    "   - **One-Hot Encoding** creates a new binary column for each category. It is typically used for nominal data (data without a natural order, like 'Red', 'Blue', 'Green').\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 - Data Wrangling II\n",
    "\n",
    "#### **Objective**:\n",
    "\n",
    "To clean and prepare the dataset by detecting inconsistencies, handling outliers, and applying normalization techniques for better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Steps**:\n",
    "\n",
    "1. **Scan Variables for Inconsistencies**:\n",
    "\n",
    "   - Use `unique()` to identify unexpected values or errors in each column, especially for categorical data.\n",
    "\n",
    "2. **Remove Outliers**:\n",
    "\n",
    "   - Detect outliers using methods like **IQR (Interquartile Range)** or **Z-score** (values beyond Â±3).\n",
    "   - Optionally, remove or cap outliers to prevent them from affecting the analysis.\n",
    "\n",
    "3. **Min-Max Normalization**:\n",
    "\n",
    "   - Scale numerical data to the range [0, 1] using the formula:\n",
    "     $$\n",
    "     X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "     $$\n",
    "   - Useful for models sensitive to feature scale.\n",
    "\n",
    "4. **Z-score Normalization**:\n",
    "\n",
    "   - Standardize the data by subtracting the mean and dividing by the standard deviation:\n",
    "     $$\n",
    "     Z = \\frac{X - \\mu}{\\sigma}\n",
    "     $$\n",
    "   - Best for data with Gaussian distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts**:\n",
    "\n",
    "- **Inconsistencies**: Check for errors or unexpected values in data.\n",
    "- **Outliers**: Extreme values that can distort analysis, detected and handled through statistical methods.\n",
    "- **Normalization**: Min-Max and Z-score techniques adjust data scale for better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Questions & Answers**:\n",
    "\n",
    "**Q1: Min-Max vs Z-score Normalization?**\\\n",
    "**A1**: Min-Max scales data to a range [0, 1], but is sensitive to outliers. Z-score standardizes data, making it less affected by outliers.\n",
    "\n",
    "**Q2: How to handle outliers?**\\\n",
    "**A2**: Use IQR or Z-score to detect outliers, then remove or cap them based on your approach.\n",
    "\n",
    "**Q3: Why check for inconsistencies?**\\\n",
    "**A3**: Inconsistencies can lead to errors in analysis and affect model accuracy, so they need to be fixed before further analysis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 3: Data Wrangling I - NBA & Iris Dataset**\n",
    "\n",
    "## **Part 1: NBA Dataset**\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Preprocessing & Removing Inconsistencies**:\n",
    "\n",
    "   - Handle missing or incorrect values (e.g., negative heights).\n",
    "   - Convert height to a consistent unit (e.g., inches to centimeters).\n",
    "\n",
    "2. **Remove Outliers**:\n",
    "\n",
    "   - Detect outliers using Z-score or IQR methods, then remove or cap them.\n",
    "\n",
    "3. **Central Tendency**:\n",
    "\n",
    "   - Calculate mean, median, and mode for numerical columns like height, age, and salary.\n",
    "\n",
    "4. **Variance**:\n",
    "\n",
    "   - Calculate variance and standard deviation to understand the spread of the data.\n",
    "\n",
    "5. **Grouping by Age**:\n",
    "\n",
    "   - Use `pd.cut()` to group players into age bins (e.g., 18-25, 26-30).\n",
    "\n",
    "### **One-line Example for pd.cut**:\n",
    "\n",
    "```python\n",
    "df['age_group'] = pd.cut(df['age'], bins=[18, 25, 30, 35, 40], labels=['18-25', '26-30', '31-35', '36-40'], right=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 2: Iris Dataset**\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Preprocessing & Removing Inconsistencies**:\n",
    "\n",
    "   - Handle missing values and ensure correct data types.\n",
    "\n",
    "2. **Remove Outliers**:\n",
    "\n",
    "   - Use Z-score or IQR methods to detect and remove outliers in numerical columns.\n",
    "\n",
    "3. **Central Tendency**:\n",
    "\n",
    "   - Calculate mean, median, and mode for sepal and petal dimensions.\n",
    "\n",
    "4. **Variance**:\n",
    "\n",
    "   - Compute variance and standard deviation for numerical columns like sepal length and petal width.\n",
    "\n",
    "5. **Group by Species**:\n",
    "\n",
    "   - Group by species and analyze measurements (e.g., mean, median) for each species.\n",
    "\n",
    "6. **Label & One-Hot Encoding**:\n",
    "\n",
    "   - Use **LabelEncoder** to convert species to numerical labels.\n",
    "   - Apply **OneHotEncoder** or `pd.get_dummies()` for categorical variables (e.g., species).\n",
    "\n",
    "### **One-line Example for OneHotEncoder**:\n",
    "\n",
    "```python\n",
    "df_encoded = pd.get_dummies(df, columns=['species'])\n",
    "```\n",
    "\n",
    "### **One-line Example for LabelEncoder**:\n",
    "\n",
    "```python\n",
    "df['species_encoded'] = le.fit_transform(df['species'])\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# from nltk import pos_tag\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import import stopwords\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "# Assignment steps\n",
    "\n",
    "# Import & download\n",
    "# open and read\n",
    "# tokenize\n",
    "# stop word removal\n",
    "# pos tagging\n",
    "# stemming\n",
    "# lemmatization\n",
    "\n",
    "#TF\n",
    "#IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark-shell\n",
    "# paste the program\n",
    "# nk -lc 9999\n",
    "# WordCount.main(Array())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DjangoTutorials-ELbGRrFZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
